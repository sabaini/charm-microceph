name: Build/Test

on:
  workflow_call:

jobs:
  lint:
    name: Lint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install dependencies
        run: python3 -m pip install tox
      - name: Run linters
        run: tox -e pep8

  unit-test:
    name: Unit tests
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install dependencies
        run: python -m pip install tox
      - name: Run tests
        run: tox -e py3

  terragrunt-validate:
    name: Terragrunt validate
    runs-on: ubuntu-latest
    needs:
      - lint
      - unit-test
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install Terraform and Terragrunt
        run: ./tests/scripts/ci_helpers.sh install_terraform_tooling
      - name: Validate Terragrunt module
        env:
          TF_VAR_model_uuid: 00000000-0000-0000-0000-000000000000
        run: ./tests/scripts/ci_helpers.sh validate_terragrunt_module

  build:
    name: Build the charm
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          channel: 5.21/stable
      - name: Build charm(s)
        id: builder
        run: |
          sudo snap install charmcraft --classic
          charmcraft pack -v
          ./rename.sh
      - name: Upload built charm
        uses: actions/upload-artifact@v4
        with:
          name: charms
          path: "*.charm"

  integration-test:
    needs:
      - lint
      - unit-test
      - build
    name: Integration tests
    runs-on: self-hosted-linux-amd64-noble-xlarge
    timeout-minutes: 150
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          channel: 5.21/stable

      - name: Install tooling
        run: |
          sudo snap install charmcraft --classic
          sudo apt-get -qq install tox

      - name: Install and bootstrap
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Run integration tests
        run: |
          set -eux
          cp ~/artifacts/microceph.charm .
          tox -e integration -- --keep-models -m smoke

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_functional_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main
          

  functional-test:
    needs:
      - lint
      - unit-test
      - build
    name: Functional test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get actionutils
        run: |
          curl -sL https://raw.githubusercontent.com/canonical/microceph/main/tests/scripts/actionutils.sh -o ~/actionutils.sh
          chmod +x ~/actionutils.sh

      - name: Clear FORWARD firewall rules
        run: ~/actionutils.sh cleaript

      - name: Install sunbeam
        run: |
          date
          sudo snap remove --purge lxd          
          sudo apt remove --purge docker.io containerd runc -y
          sudo rm -rf /run/containerd

          sudo nft chain ip filter FORWARD '{policy accept;}'
          sudo snap install juju --channel 3.6/stable

          sudo snap install openstack --channel 2024.1
          sudo snap connect openstack:juju-bin juju:juju-bin

          sunbeam prepare-node-script --bootstrap | bash -x
          sg snap_daemon "sunbeam cluster bootstrap --accept-defaults --topology single --database single"
          sg snap_daemon "sunbeam cluster list"
          juju status -m admin/controller
          juju status -m openstack
          juju switch sunbeam-controller:admin/openstack-machines
          sg snap_daemon "sunbeam configure --accept-defaults --openrc demo-openrc"
          date

      - name: Upgrade to locally built charm
        run: |
          juju refresh microceph --path ~/artifacts/microceph.charm
          juju add-unit microceph --to 0
          # Wait for unit to show up
          for i in $( seq 1 20 ) ; do
              if juju status -m admin/controller | grep -E 'microceph/.*active.*idle' ; then
                  echo "microceph unit active"; break
              else
                  echo -n "." ; sleep 4
              fi
          done
          sudo snap list
          juju status

      - name: Add disks
        run: |
          set -eux
          juju add-storage microceph/0 osd-standalone=loop,2G,3
          sleep 20 # extra time for juju
          ~/actionutils.sh wait_for_osds 3
          sudo microceph.ceph -s

      - name: Test RGW
        run: |
          set -eux
          juju config microceph enable-rgw="*"
          ~/actionutils.sh wait_for_rgw 1
          ~/actionutils.sh testrgw

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_sunbeam_and_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_functional_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  juju-cluster-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju cluster test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          date
          juju deploy -n 3 ~/artifacts/microceph.charm --config default-pool-size=1
          ./tests/scripts/ci_helpers.sh wait_for_microceph_bootstrap
          date

      - name: Show Juju status
        run: |
          set -eux
          juju status

      - name: Add disks
        run: |
          set -eux
          juju run microceph/0 add-osd loop-spec=4G,1 --wait=2m
          juju run microceph/1 add-osd loop-spec=4G,1 --wait=2m
          juju run microceph/2 add-osd loop-spec=4G,1 --wait=2m

      - name: Remove Unit
        run: |
          set -x
          ./tests/scripts/ci_helpers.sh remove_unit_wait "microceph/2"

      - name: Test mon addresses
        run: |
          curl="sudo curl -s --unix-socket /var/snap/microceph/common/state/control.socket"
          mons=$( ( juju ssh microceph/0 "$curl http://localhost/1.0/services/mon | \
            jq -r '.metadata.addresses[]' | wc -l" ) )
          if [[ $mons -ne 2 ]] ; then
            echo "Expected 2 MONs, got $mons. MON status: "
            $curl http://localhost/1.0/services/mon
            exit 1
          fi

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_cluster_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  juju-cluster-maintenance-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju cluster maintenance test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          date
          # FIXME: change to squid/stable when maintenance feature is landed
          juju deploy -n 4 ~/artifacts/microceph.charm --config snap-channel=squid/edge
          ./tests/scripts/ci_helpers.sh wait_for_microceph_bootstrap
          date

      - name: Show Juju status
        run: |
          set -eux
          juju status

      - name: Enable one extra ceph mon for redundancy
        run: |
          set -eux

          echo "Make sure all units have ceph monitor enabled."

          for i in 0 1 2 3 ; do
            juju ssh microceph/$i -- sudo microceph enable mon || true
          done

      - name: Add disks
        run: |
          set -eux
          juju run microceph/0 add-osd loop-spec=4G,1 --wait=2m
          juju run microceph/1 add-osd loop-spec=4G,1 --wait=2m
          juju run microceph/2 add-osd loop-spec=4G,1 --wait=2m
          juju run microceph/3 add-osd loop-spec=4G,1 --wait=2m

      - name: Show Ceph status
        run: |
          set -eux
          juju ssh microceph/0 -- sudo microceph status
          juju ssh microceph/0 -- sudo microceph.ceph -s

      - name: Assert Ceph is in healthy state before running test
        run: |
          set -eux
          juju ssh microceph/0 -- sudo microceph.ceph health | grep HEALTH_OK

      - name: Dry run test
        run: |
          set -eux

          echo "Test dry running enter-maintenance and exit-maintenance actions."

          result=$(juju run microceph/0 exit-maintenance dry-run=true --format json)
          echo $result

          action_counts=$(echo $result | jq '."microceph/0".results.actions | length')
          if [ "$action_counts" != "3" ] ; then
            echo "Expect dry run exit-maintenance to produce 3 steps."
            exit 1
          fi

          result=$(juju run microceph/0 enter-maintenance dry-run=true --format json)
          echo $result

          action_counts=$(echo $result | jq '."microceph/0".results.actions | length')
          if [ "$action_counts" != "4" ] ; then
            echo "Expect dry run enter-maintenance to produce 3 steps."
            exit 1
          fi

      - name: Test enter maintenance mode okay
        run: |
          set -eux

          echo "Test running enter-maintenance action."

          result=$(juju run microceph/3 enter-maintenance stop-osds=true --format json)
          echo $result

          status=$(echo $result | jq '."microceph/3".results.status' -r )
          if [ "$status" != "success" ] ; then
            echo "Expect enter-maintenance to succeed."
            exit 1
          fi

          if ! $(juju ssh microceph/3 -- sudo microceph.ceph osd dump | grep noout > /dev/null 2>&1) ; then
            echo "Expect osd noout is set"
            exit 1
          fi

          if ! $(juju ssh microceph/3 -- snap services microceph.osd | grep inactive > /dev/null 2>&1) ; then
            echo "Expect osd service is inactive"
            exit 1
          fi

          if ! $(juju ssh microceph/3 -- snap services microceph.osd | grep disabled > /dev/null 2>&1) ; then
            echo "Expect osd service is disabled"
            exit 1
          fi

      - name: Test exit maintenance mode okay
        run: |
          set -eux

          echo "Test running exit-maintenance action."

          result=$(juju run microceph/3 exit-maintenance --format json)
          echo $result

          status=$(echo $result | jq '."microceph/3".results.status' -r )
          if [ "$status" != "success" ] ; then
            echo "Expect exit-maintenance to succeed."
            exit 1
          fi

          if $(juju ssh microceph/3 -- sudo microceph.ceph osd dump | grep noout > /dev/null 2>&1) ; then
            echo "Expect osd noout is unset"
            exit 1
          fi

          if ! $(juju ssh microceph/3 -- snap services microceph.osd | grep active > /dev/null 2>&1) ; then
            echo "Expect osd service is active"
            exit 1
          fi

          if ! $(juju ssh microceph/3 -- snap services microceph.osd | grep enabled > /dev/null 2>&1) ; then
            echo "Expect osd service is enabled"
            exit 1
          fi

      - name: Remove Unit
        run: |
          set -x
          ./tests/scripts/ci_helpers.sh remove_unit_wait "microceph/3"

      - name: Test enter maintenance mode fail
        run: |
          set -eux

          echo "Test running enter-maintenance action (fail case)."

          result=$(juju run microceph/2 enter-maintenance stop-osds=true --format json)
          echo $result

          has_errors=$(echo $result | jq '."microceph/2".results.errors != ""')
          if [ "$has_errors" != "true" ] ; then
            echo "Expect enter-maintenance to fail with error message because there is only 3 nodes."
            exit 1
          fi

          status_failure=$(echo $result | jq '."microceph/2".results.status == "failure"')
          if [ "$status_failure" != "true" ] ; then
            echo "Expect enter-maintenance to fail with status=failure."
            exit 1
          fi

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_cluster_maintenance_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  juju-wipe-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju wipe test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Run functional tests
        run: |
          set -eux
          cp ~/artifacts/microceph.charm .
          tox -e functional -- --keep-models

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_wipe_test_logs
          path: logs
          retention-days: 30

  juju-single-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju single test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          set -eux
          date
          # constraints to deploy on virtual machine.
          juju deploy ~/artifacts/microceph.charm --storage osd-standalone='2G,6' --constraints="virt-type=virtual-machine root-disk=24G mem=8G"
          # wait for charm to bootstrap and OSD devices to enroll.
          juju wait-for unit microceph/0 --timeout '30m' --query='workload-message=="(workload) charm is ready"'
          bash ./tests/scripts/ci_helpers.sh check_osd_count microceph/0 6
          date

      - name: Remove Juju Storage
        run: |
          set -eux
          juju detach-storage osd-standalone/0
          juju wait-for unit microceph/0 --timeout '20m' --query='workload-message=="(workload) charm is ready"'
          # wait before completely removing the storage.
          sleep 1m
          juju remove-storage osd-standalone/0
          bash tests/scripts/ci_helpers.sh check_osd_count microceph/0 5
          date

      - name: Add another OSD
        run: |
          set -eux
          juju add-storage microceph/0 osd-standalone='2G,1'
          juju wait-for unit microceph/0 --timeout '20m' --query='workload-message=="(workload) charm is ready"'
          bash tests/scripts/ci_helpers.sh check_osd_count microceph/0 6
          date

      - name: Add and relate RadosGW charm
        run: |
          set -eux
          juju deploy ch:ceph-radosgw --base ubuntu@24.04 --channel squid/edge
          sleep 30
          juju integrate ceph-radosgw microceph
          juju wait-for unit ceph-radosgw/0 --query='workload-status=="active"' --timeout=20m
          for i in $(seq 1 20); do
            rgwpools=$(juju ssh microceph/0 -- "sudo microceph.ceph osd pool ls")
            if echo "$rgwpools" | grep -E 'rgw' ; then
              echo "Found RadosGW pools"
              break
            else
              echo -n '.'
              sleep 60
            fi
          done
          rgwpools=$(juju ssh microceph/0 -- "sudo microceph.ceph osd pool ls")
          if ! echo "$rgwpools" | grep -E "rgw" ; then
            echo "No RadosGW pools were created"
            juju ssh microceph/0 -- "sudo microceph.ceph osd pool ls"
            juju ssh microceph/0 -- "sudo microceph.ceph -s"
            exit 1
          fi

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_single_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main


  snap-upgrade-test:
    needs:
      - lint
      - unit-test
      - build
    name: snap upgrade test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          date
          juju deploy -n 3 ~/artifacts/microceph.charm --config snap-channel=reef/stable
          ./tests/scripts/ci_helpers.sh wait_for_microceph_bootstrap
          date

      - name: Show Juju status
        run: |
          set -eux
          juju status

      - name: Test non-existing channel
        run: |
          set -eux
          juju status
          juju config microceph snap-channel="non/exist"
          juju wait-for application microceph --query='status=="blocked"'
          juju status
          juju status blocked | egrep '^microceph/.*Cannot upgrade.*to non/exist'
          # set back
          juju config microceph snap-channel=reef/stable
          sleep 10

      - name: Test upgrading with unhealthy cluster blocks
        run: |
          set -eux
          juju config microceph snap-channel=reef/edge
          sleep 10
          juju wait-for application microceph --query='status=="blocked"'
          juju status
          juju status blocked | egrep '^microceph/.*Cannot upgrade, ceph health not ok'
          # Should be in WARN as we don't have OSDs
          juju ssh microceph/0 -- sudo ceph -s
          juju config microceph snap-channel=reef/stable
          juju wait-for application microceph --query='name=="microceph" && (status=="active" || status=="idle")' --timeout=20m

      - name: Add disks for health
        run: |
          set -eux
          for i in 0 1 2 ; do juju run microceph/$i add-osd loop-spec=4G,1 ; done
          sleep 60
          juju ssh microceph/0 -- sudo ceph -s

      - name: Test successful upgrade
        run: |
          set -eux
          date
          target="squid/stable"
          juju config microceph snap-channel=$target
          sleep 60
          # Initial wait
          juju wait-for application microceph --query='forEach(units, unit => unit.workload-status=="active" && unit.agent-status=="idle")' --timeout=30m || true
          # Get status
          juju ssh microceph/0 -- sudo ceph -s
          juju ssh microceph/0 -- sudo microceph cluster list
          juju status
          # Fail if goal status not reached
          juju wait-for application microceph --query='forEach(units, unit => unit.workload-status=="active" && unit.agent-status=="idle")' --timeout=2m
          date
          # Test if we have expected snap releases
          juju exec -a microceph -- sudo snap info microceph
          cnt=$( juju exec -a microceph -- sudo snap info microceph | egrep "^tracking:" | fgrep -c $target )
          if [[ "$cnt" -ne "3" ]] ; then
            echo "Fail: expected 3 nodes at $target, got $cnt"
            exit -1
          fi

      - name: Test the set-pool-size action
        run: |
          set -eux
          juju ssh microceph/0 -- sudo ceph osd pool create mypool1
          juju ssh microceph/0 -- sudo ceph osd pool create mypool2
          juju run microceph/0 set-pool-size pools=mypool1,mypool2 size=1
          juju ssh microceph/0 -- sudo ceph osd pool get mypool1 size | fgrep -x "size: 1"
          juju ssh microceph/0 -- sudo ceph osd pool get mypool2 size | fgrep -x "size: 1"

      - name: Test that downgrade blocks
        run: |
          set -eux
          juju config microceph snap-channel=reef/stable
          juju wait-for application microceph --query='status=="blocked"'
          juju status
          juju status blocked | egrep '^microceph/.*Cannot upgrade.*to reef/stable'

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_upgrade_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  juju-network-spaces-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju Network spaces test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd since there were some recurring issues in recent releases.
          channel: 5.21/stable

      - name: Prepare Juju and LXD
        run: ./tests/scripts/ci_helpers.sh prepare_environment

      - name: Configure Juju spaces
        run: |
          # Seed multi-interface profile with netplan config.
          ./tests/scripts/ci_helpers.sh seed_lxd_profile ./tests/scripts/assets/lxd-preseed-cloud-init.yaml
          # Configure Juju spaces
          ./tests/scripts/ci_helpers.sh setup_juju_spaces

      - name: Prepare 3 virtual machines
        run: ./tests/scripts/ci_helpers.sh prepare_3_vms

      - name: Install MicroCeph charm
        run: |
          set -eux
          date
          mv ~/artifacts/microceph.charm ./tests/scripts/assets/
          juju deploy microceph --bind "cluster=cluster public=alpha" --storage osd-standalone=2G,1 -n 3 --to 0,1,2
          # wait for charm to bootstrap and OSD devices to enroll.
          juju status
          juju wait-for application microceph --query='name=="microceph" && (status=="active" || status=="idle")' --timeout=40m
          bash ./tests/scripts/ci_helpers.sh check_osd_count microceph/0 3
          date

      - name: Validate the network configurations.
        run: ./tests/scripts/ci_helpers.sh verify_juju_spaces_config

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_network_spaces_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  juju-mds-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju mds test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          set -eux
          date
          # constraints to deploy on virtual machine.
          juju deploy ~/artifacts/microceph.charm --storage osd-standalone='2G,6' --constraints="virt-type=virtual-machine root-disk=24G mem=8G"
          # wait for charm to bootstrap and OSD devices to enroll.
          juju wait-for unit microceph/0 --timeout '30m' --query='workload-message=="(workload) charm is ready"'
          bash ./tests/scripts/ci_helpers.sh check_osd_count microceph/0 6
          date

      - name: Add and relate CephFS charm
        run: |
          set -eux
          juju deploy ch:ceph-fs --base ubuntu@24.04 --channel squid/edge
          # wait for charm to bootstrap.
          juju wait-for unit ceph-fs/0 --query='workload-status=="blocked"' --timeout=30m
          juju integrate ceph-fs:ceph-mds microceph:mds
          # wait for filesystem creation
          juju wait-for application ceph-fs --query='name=="ceph-fs" && (status=="active" || status=="idle")' --timeout=10m
          filesystems=$(juju ssh microceph/0 -- "sudo microceph.ceph fs ls")
          if ! echo "$filesystems" | grep -E "ceph-fs" ; then
            echo "Failed to create the ceph-fs filesystem"
            juju ssh microceph/0 -- "sudo microceph.ceph fs ls"
            exit 1
          fi
          # check if the ceph-mds daemon is running
          juju ssh ceph-fs/0 -- 'sudo systemctl is-active ceph-mds@$HOSTNAME.service --quiet'

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_mds_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  juju-remote-relation-test:
    needs:
      - lint
      - unit-test
      - build
    name: Remote Integration test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          set -eux
          date
          # Deploy 2 microceph clusters named primary and secondary
          juju deploy ~/artifacts/microceph.charm primary --storage osd-standalone='2G,3' --constraints="virt-type=virtual-machine root-disk=25G mem=8G"
          juju deploy ~/artifacts/microceph.charm secondary --storage osd-standalone='2G,3' --constraints="virt-type=virtual-machine root-disk=25G mem=8G"
          # wait for charm to bootstrap and OSD devices to enroll.
          juju wait-for unit primary/0 --timeout '30m' --query='workload-message=="(workload) charm is ready"'
          juju wait-for unit secondary/0 --timeout '15m' --query='workload-message=="(workload) charm is ready"'
          bash ./tests/scripts/ci_helpers.sh check_osd_count primary/0 3
          bash ./tests/scripts/ci_helpers.sh check_osd_count secondary/0 3
          date

      - name: Add remote integration
        run: |
          set -eux
          juju integrate primary:remote-provider secondary:remote-requirer

          # wait for the applications to go to blocked state
          juju wait-for unit primary/0 --query='workload-status=="blocked"' --timeout=10m
          juju wait-for unit secondary/0 --query='workload-status=="blocked"' --timeout=10m

          juju config primary site-name="primary"
          juju config secondary site-name="secondary"


          juju wait-for unit primary/0 --query='workload-status=="active"' --timeout=10m
          juju wait-for unit secondary/0 --query='workload-status=="active"' --timeout=10m

          juju status --relations

      - name: Wait for token exchange and remote enlisting
        run: |
          set -eux
          bash ./tests/scripts/remote_integration_helpers.sh wait_for_remote_enlistment primary/0 secondary
          bash ./tests/scripts/remote_integration_helpers.sh wait_for_remote_enlistment secondary/0 primary

      - name: Scale up the primary cluster
        run: |
          set -eux
          juju add-unit primary -n 1
          # wait for new unit to be active/idle
          juju wait-for unit primary/1 --query='workload-status=="active"' --timeout=20m

      - name: Re-verify remote listings
        run: |
          set -eux
          bash ./tests/scripts/remote_integration_helpers.sh wait_for_remote_enlistment primary/0 secondary
          bash ./tests/scripts/remote_integration_helpers.sh wait_for_remote_enlistment primary/1 secondary
          bash ./tests/scripts/remote_integration_helpers.sh wait_for_remote_enlistment secondary/0 primary

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_mds_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  juju-storage-cluster-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju multi node storage test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          date
          mv ~/artifacts/microceph.charm ./microceph.charm
          juju deploy ./tests/bundles/multi_node_juju_storage.yaml
          # wait for charm to bootstrap and OSD devices to enroll.
          ./tests/scripts/ci_helpers.sh wait_for_microceph_bootstrap
          date

      - name: Show Juju status
        run: |
          set -eux
          juju status

      - name: Remove Unit microceph/2
        run: |
          set -x
          ./tests/scripts/ci_helpers.sh ensure_osd_count_on_host "^juju-.*\-2$" 3
          ./tests/scripts/ci_helpers.sh remove_unit_wait "microceph/2"

          # sleep for some time
          sleep 1m

          ./tests/scripts/ci_helpers.sh ensure_osd_count_on_host "^juju-.*\-2$" 0

      - name: Add another unit
        run: |
          set -x
          juju add-unit microceph -n 1
          sleep 60s
          juju wait-for unit microceph/3 --query='workload-message=="(workload) charm is ready"' --timeout=20m

           # "juju" substring is a part of all hosts, so it will fetch total OSD count
          ./tests/scripts/ci_helpers.sh ensure_osd_count_on_host "juju" 9

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_juju_storage_cluster_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  cos-integration-test:
    needs:
      - lint
      - unit-test
      - build
    name: Charm MicroCeph COS integration test
    runs-on: self-hosted-linux-amd64-noble-xlarge
    steps:
      - name: Download charm
        uses: actions/download-artifact@v4
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: |
          ./tests/scripts/ci_helpers.sh install_deps
          sudo snap install microk8s --channel 1.32-strict/stable

      - name: Cleanup Docker
        run: ./tests/scripts/ci_helpers.sh cleanup_docker

      - name: Bootstrap K8s
        run: ./tests/scripts/ci_helpers.sh bootstrap_k8s

      - name: Bootstrap k8s controller
        run: ./tests/scripts/ci_helpers.sh bootstrap_k8s_controller

      - name: Deploy cos lite
        run: ./tests/scripts/ci_helpers.sh deploy_cos

      - name: Check http endpoints
        run: ./tests/scripts/ci_helpers.sh check_http_endpoints_up

      - name: Deploy MicroCeph charm over LXD
        run: ./tests/scripts/ci_helpers.sh deploy_microceph

      - name: Deploy and Integrate grafana-agent
        run: ./tests/scripts/ci_helpers.sh deploy_grafana_agent

      - name: Test Metrics and Dashboards
        run: ./tests/scripts/ci_helpers.sh verify_o11y_services

      - name: Show Juju status
        run: |
          set -eux
          juju status

      - name: Collect logs
        if: failure()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs || true

      - name: Upload logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: microceph_cos_integration_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main
