name: Build/Test

on:
  workflow_call:

jobs:
  lint:
    name: Lint
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install dependencies
        run: python3 -m pip install tox
      - name: Run linters
        run: tox -e pep8

  unit-test:
    name: Unit tests
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install dependencies
        run: python -m pip install tox
      - name: Run tests
        run: tox -e py3

  build:
    name: Build the charm
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          channel: 5.20/stable
      - name: Build charm(s)
        id: builder
        run: |
          sudo snap install charmcraft --classic
          charmcraft pack -v
          ./rename.sh
      - name: Upload built charm
        uses: actions/upload-artifact@v3
        with:
          name: charms
          path: "*.charm"

  functional-test:
    needs:
      - lint
      - unit-test
      - build
    name: Functional test
    runs-on: [self-hosted, xlarge]
    steps:

      - name: Download charm
        uses: actions/download-artifact@v3
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install sunbeam
        run: |
          date
          sudo snap install openstack --channel 2023.2
          sunbeam prepare-node-script | bash -x
          sudo snap refresh juju --channel 3.4/stable
          sg snap_daemon "sunbeam cluster bootstrap --accept-defaults"
          sg snap_daemon "sunbeam cluster list"
          juju status -m admin/controller
          juju status -m openstack
          date

      - name: Get actionutils
        run: |
          curl -sL https://raw.githubusercontent.com/canonical/microceph/main/tests/scripts/actionutils.sh -o ~/actionutils.sh
          chmod +x ~/actionutils.sh

      - name: Upgrade to locally built charm
        run: |
          juju refresh microceph --path ~/artifacts/microceph.charm
          juju add-unit -m admin/controller microceph --to 0
          # Wait for unit to show up
          for i in $( seq 1 20 ) ; do
              if juju status -m admin/controller | grep -E 'microceph/.*active.*idle' ; then
                  echo "microceph unit active"; break
              else
                  echo -n "." ; sleep 4
              fi
          done
          sudo snap list
          juju status -m admin/controller
          # TODO: We can't upgrade the snap via the charm yet, do it by hand
          sudo snap refresh microceph --channel=reef/edge

      - name: Add disks
        run: |
          set -eux
          juju run microceph/0 add-osd loop-spec=4G,3
          sudo microceph.ceph -s
          ~/actionutils.sh wait_for_osds 3
          sudo microceph.ceph -s

      - name: Enable RGW
        run: |
          set -eux
          juju config microceph enable-rgw="*"
          ~/actionutils.sh wait_for_rgw 1

      - name: Collect logs
        if: always()
        run: ./tests/scripts/ci_helpers.sh collect_sunbeam_and_microceph_logs

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: microceph_juju_functional_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  juju-cluster-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju cluster test
    runs-on: [self-hosted, xlarge]
    steps:

      - name: Download charm
        uses: actions/download-artifact@v3
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          date
          juju deploy -n 3 ~/artifacts/microceph.charm --config default-pool-size=1
          ./tests/scripts/ci_helpers.sh wait_for_microceph_bootstrap
          date

      - name: Show Juju status
        run: |
          set -eux
          juju status

      - name: Add disks
        run: |
          set -eux
          juju run microceph/0 add-osd loop-spec=4G,1 --wait=2m
          juju run microceph/1 add-osd loop-spec=4G,1 --wait=2m
          juju run microceph/2 add-osd loop-spec=4G,1 --wait=2m

      - name: Remove Unit
        run: |
          set -x
          juju remove-unit microceph/2 --no-prompt
          # wait and check if the unit is still present.
          for i in $(seq 1 40); do
            res=$( ( juju status | grep -cF "microceph/2" ) || true )
            if [[ $res -gt 0 ]] ; then
              echo -n '.'
              sleep 5
            else
              echo "Unit removed successfully"
              break
            fi
          done
          # fail if unit still present.
          if [[ $res -gt 0 ]] ; then
            echo "Unit still present"
            juju status
            exit 1
          fi

      - name: Collect logs
        if: always()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: microceph_juju_cluster_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  juju-single-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju single test
    runs-on: [self-hosted, xlarge]
    steps:

      - name: Download charm
        uses: actions/download-artifact@v3
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          set -eux
          date
          # constraints to deploy on virtual machine.
          juju deploy ~/artifacts/microceph.charm --storage osd-standalone='2G,3' --constraints="virt-type=virtual-machine root-disk=50G mem=8G"
          # wait for charm to bootstrap and OSD devices to enroll.
          juju wait-for unit microceph/0 --timeout '30m' --query='workload-message=="(workload) charm is ready"'
          bash ./tests/scripts/ci_helpers.sh check_osd_count microceph/0 3
          date

      - name: Remove Juju Storage
        run: |
          set -eux
          juju detach-storage osd-standalone/0
          juju wait-for unit microceph/0 --query='workload-status=="blocked"'
          # wait before completely removing the storage.
          sleep 1m
          juju remove-storage osd-standalone/0
          bash tests/scripts/ci_helpers.sh check_osd_count microceph/0 2
          date

      - name: Add another OSD
        run: |
          set -eux
          juju add-storage microceph/0 osd-standalone='2G,1'
          juju wait-for unit microceph/0 --timeout '20m' --query='workload-message=="(workload) charm is ready"'
          bash tests/scripts/ci_helpers.sh check_osd_count microceph/0 3
          date

      - name: Add and relate RadosGW charm
        run: |
          set -eux
          juju deploy ch:ceph-radosgw
          sleep 120
          juju integrate ceph-radosgw microceph
          sleep 420
          for i in $(seq 1 20); do
            rgwpools=$(juju ssh microceph/0 -- "sudo microceph.ceph osd pool ls")
            if echo "$rgwpools" | grep -E 'rgw' ; then
              echo "Found RadosGW pools"
              break
            else
              echo -n '.'
              sleep 60
            fi
          done
          rgwpools=$(juju ssh microceph/0 -- "sudo microceph.ceph osd pool ls")
          if ! echo "$rgwpools" | grep -E "rgw" ; then
            echo "No RadosGW pools were created"
            juju ssh microceph/0 -- "sudo microceph.ceph osd pool ls"
            juju ssh microceph/0 -- "sudo microceph.ceph -s"
            exit 1
          fi

  juju-upgrade-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju upgrade test
    runs-on: [self-hosted, xlarge]
    steps:

      - name: Download charm
        uses: actions/download-artifact@v3
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          date
          juju deploy -n 3 ~/artifacts/microceph.charm --config snap-channel=quincy/stable
          ./tests/scripts/ci_helpers.sh wait_for_microceph_bootstrap
          date

      - name: Show Juju status
        run: |
          set -eux
          juju status

      - name: Test non-existing channel
        run: |
          set -eux
          juju status
          juju config microceph snap-channel="non/exist"
          juju wait-for application microceph --query='status=="blocked"'
          juju status
          juju status blocked | egrep '^microceph/.*Cannot upgrade.*to non/exist'
          # set back
          juju config microceph snap-channel=quincy/stable
          sleep 10

      - name: Test upgrading with unhealthy cluster blocks
        run: |
          set -eux
          juju config microceph snap-channel=quincy/edge
          sleep 10
          juju wait-for application microceph --query='status=="blocked"'
          juju status
          juju status blocked | egrep '^microceph/.*Cannot upgrade, ceph health not ok'
          # Should be in WARN as we don't have OSDs
          juju ssh microceph/0 -- sudo ceph -s
          juju config microceph snap-channel=quincy/stable
          juju wait-for application microceph --query='name=="microceph" && (status=="active" || status=="idle")' --timeout=20m

      - name: Add disks for health
        run: |
          set -eux
          for i in 0 1 2 ; do juju run microceph/$i add-osd loop-spec=4G,1 ; done
          sleep 60
          juju ssh microceph/0 -- sudo ceph -s

      - name: Test successful upgrade
        run: |
          set -eux
          date
          juju config microceph snap-channel=reef/candidate
          sleep 60
          # Initial wait
          juju wait-for application microceph --query='forEach(units, unit => unit.workload-status=="active" && unit.agent-status=="idle")' --timeout=30m || true
          # Get status
          juju ssh microceph/0 -- sudo ceph -s
          juju status
          # Fail if goal status not reached
          juju wait-for application microceph --query='forEach(units, unit => unit.workload-status=="active" && unit.agent-status=="idle")' --timeout=2m
          date

      - name: Test the set-pool-size action
        run: |
          set -eux
          juju ssh microceph/0 -- sudo ceph osd pool create mypool1
          juju ssh microceph/0 -- sudo ceph osd pool create mypool2
          juju run microceph/0 set-pool-size pools=mypool1,mypool2 size=1
          juju ssh microceph/0 -- sudo ceph osd pool get mypool1 size | fgrep -x "size: 1"
          juju ssh microceph/0 -- sudo ceph osd pool get mypool2 size | fgrep -x "size: 1"

      - name: Test that downgrade blocks
        run: |
          set -eux
          juju config microceph snap-channel=quincy/stable
          juju wait-for application microceph --query='status=="blocked"'
          juju status
          juju status blocked | egrep '^microceph/.*Cannot upgrade.*to quincy/stable'

      - name: Collect logs
        if: always()
        run: ./tests/scripts/ci_helpers.sh collect_microceph_logs

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: microceph_juju_upgrade_test_logs
          path: logs
          retention-days: 30

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main

  juju-network-spaces-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju Network spaces test
    runs-on: [self-hosted, xlarge]
    steps:

      - name: Download charm
        uses: actions/download-artifact@v3
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd since there were some recurring issues in recent releases.
          channel: 5.21/stable

      - name: Seed preliminary LXD profile
        run: ./tests/scripts/ci_helpers.sh seed_lxd_profile ./tests/scripts/assets/lxd-preseed.yaml

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_juju_simple

      - name: Configure Juju spaces
        run: |
          # Seed multi-interface profile with netplan config.
          ./tests/scripts/ci_helpers.sh seed_lxd_profile ./tests/scripts/assets/lxd-preseed-cloud-init.yaml
          # Configure Juju spaces
          ./tests/scripts/ci_helpers.sh setup_juju_spaces

      - name: Install MicroCeph charm
        run: |
          set -eux
          date
          mv ~/artifacts/microceph.charm ./tests/scripts/assets/
          juju deploy ./tests/scripts/assets/juju-spaces-bundle.yaml
          # wait for charm to bootstrap and OSD devices to enroll.
          juju wait-for unit microceph/0 --query='workload-message=="(workload) charm is ready"' --timeout=20m
          bash ./tests/scripts/ci_helpers.sh check_osd_count microceph/0 3
          date

      - name: Validate the network configurations.
        run: ./tests/scripts/ci_helpers.sh verify_juju_spaces_config

  juju-mds-test:
    needs:
      - lint
      - unit-test
      - build
    name: Juju mds test
    runs-on: [self-hosted, xlarge]
    steps:

      - name: Download charm
        uses: actions/download-artifact@v3
        with:
          name: charms
          path: ~/artifacts/

      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Setup LXD
        uses: canonical/setup-lxd@v0.1.1
        with:
          # pin lxd to LTS release.
          channel: 5.21/stable

      - name: Install dependencies
        run: ./tests/scripts/ci_helpers.sh install_deps

      - name: Install MicroCeph charm
        run: |
          set -eux
          date
          # constraints to deploy on virtual machine.
          juju deploy ~/artifacts/microceph.charm --storage osd-standalone='2G,3' --constraints="virt-type=virtual-machine root-disk=50G mem=8G"
          # wait for charm to bootstrap and OSD devices to enroll.
          juju wait-for unit microceph/0 --timeout '30m' --query='workload-message=="(workload) charm is ready"'
          bash ./tests/scripts/ci_helpers.sh check_osd_count microceph/0 3
          date

      - name: Add and relate CephFS charm
        run: |
          set -eux
          juju deploy ch:ceph-fs
          # wait for charm to bootstrap.
          juju wait-for unit ceph-fs/0 --query='workload-status=="blocked"' --timeout=30m
          juju integrate ceph-fs:ceph-mds microceph:mds
          # wait for filesystem creation
          juju wait-for application ceph-fs --query='name=="ceph-fs" && (status=="active" || status=="idle")' --timeout=10m
          filesystems=$(juju ssh microceph/0 -- "sudo microceph.ceph fs ls")
          if ! echo "$filesystems" | grep -E "ceph-fs" ; then
            echo "Failed to create the ceph-fs filesystem"
            juju ssh microceph/0 -- "sudo microceph.ceph fs ls"
            exit 1
          fi
          # check if the ceph-mds daemon is running
          juju ssh ceph-fs/0 -- 'sudo systemctl is-active ceph-mds@$HOSTNAME.service --quiet'

      - name: Setup tmate session
        if: ${{ failure() && runner.debug }}
        uses: canonical/action-tmate@main
